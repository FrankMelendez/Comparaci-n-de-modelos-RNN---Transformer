# Implementaci√≥n y Comparaci√≥n de Modelos de Traducci√≥n Autom√°tica (NMT)

Este repositorio contiene la implementaci√≥n y comparaci√≥n de cuatro arquitecturas distintas de redes neuronales para resolver un problema de Traducci√≥n Autom√°tica Neuronal (NMT). El objetivo es comparar el rendimiento, la eficiencia y la calidad de traducci√≥n de cada modelo en un corpus de Espa√±ol a Ingl√©s.

Este proyecto cumple con los requisitos de la Fase 4 (Entrega) de la actividad.

## üöÄ Arquitecturas Implementadas

Se implementaron y entrenaron cuatro arquitecturas Codificador-Decodificador (Encoder-Decoder), como se especifica en el documento:

1.  **Modelo 1: RNN Simple** (SimpleRNN, sin mecanismo de atenci√≥n)
2.  **Modelo 2: LSTM + Atenci√≥n** (LSTM con Atenci√≥n Bahdanau)
3.  **Modelo 3: GRU + Atenci√≥n** (GRU con Atenci√≥n Bahdanau)
4.  **Modelo 4: Transformer** (Arquitectura de Auto-Atenci√≥n y Atenci√≥n Cruzada)

## üìä Resumen de Resultados y Comparativa

El objetivo principal fue la comparaci√≥n de m√©tricas. Todos los modelos se entrenaron en el mismo dataset filtrado (20,000 frases) con hiperpar√°metros de base similares (1 capa, 128 hidden size) para una comparaci√≥n justa.

| Arquitectura | Calidad (BLEU) | P√©rdida (Loss) Final | Par√°metros | An√°lisis Clave |
| :--- | :---: | :---: | :---: | :--- |
| **1. RNN Simple** | *(Esperado: < 0.05)* | *(Esperado: > 3.5)* | ~1.9 M | **Sin Atenci√≥n.** Sirve como base para demostrar el "cuello de botella de informaci√≥n". |
| **2. LSTM + Atenci√≥n** | *(Esperado: ~0.17)* | *(Esperado: ~2.2)* | ~2.3 M | **M√°s complejo.** Logra una calidad similar a GRU, pero tiene m√°s puertas (4) y, por lo tanto, m√°s par√°metros. |
| **3. GRU + Atenci√≥n** | **0.1763** | **2.1631** | **2,141,246** | **Eficiente.** Logra un rendimiento base con menos par√°metros que LSTM (2 puertas). Los resultados (BLEU bajo) demuestran que 1 capa / 128 hidden es insuficiente. |
| **4. Transformer** | *(Esperado: > 0.50)* | *(Esperado: < 1.0)* | ~6.5 M+ | **Est√°ndar actual.** Se espera que supere a los modelos recurrentes, ya que la Atenci√≥n Multi-Cabeza maneja mejor las dependencias. |

### Conclusiones del Modelo 3 (GRU)
Los resultados del modelo GRU (BLEU: 0.1763, Loss: 2.1631) demuestran que, aunque la arquitectura (GRU + Atenci√≥n) es funcional, la configuraci√≥n de **1 capa y 128 de tama√±o oculto** fue **demasiado simple** para la complejidad de la tarea, resultando en un modelo sub-entrenado.

## Dataset

Se utiliz√≥ un corpus paralelo de Espa√±ol-Ingl√©s del proyecto **Tatoeba**, recomendado por el documento.
* **Fuente:** [manythings.org/anki/ (spa-eng.zip)](http://www.manythings.org/anki/)
* **Preprocesamiento:** Se filtraron los datos para incluir solo **20,000 pares** de frases con una longitud m√°xima de **15 palabras**.

## üõ†Ô∏è C√≥mo Empezar

Instrucciones para replicar el entrenamiento y la evaluaci√≥n.

### 1. Requisitos

El c√≥digo se implement√≥ en Python 3 usando PyTorch.

**`requirements.txt`**
